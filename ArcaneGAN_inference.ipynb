{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ArcaneGAN inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonbaer/googlecolab/blob/master/ArcaneGAN_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzYVMxTPTdmw"
      },
      "source": [
        "An inference notebook for [ArcaneGAN v0.4](https://github.com/Sxela/ArcaneGAN/releases/tag/v0.4).\n",
        "Made by [Alex Spirin](https://twitter.com/devdef)\n",
        "\n",
        "If you like what I'm doing you can tip me [here](https://donationalerts.com/r/derplearning) or follow on [Patreon](https://www.patreon.com/sxela)\n",
        "\n",
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=sxela_arcanegan)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9ikXFtITiuK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "644fa765-c35d-486a-b2cb-b0e58aae1574"
      },
      "source": [
        "#@title This colab is distributed under the MIT license\n",
        "\"\"\"MIT License\n",
        "\n",
        "Copyright (c) 2021 Alex Spirin\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'MIT License\\n\\nCopyright (c) 2021 Alex Spirin\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXqfcKRpS5Bi"
      },
      "source": [
        "#@title Install and download. Run once.\n",
        "#release v0.2\n",
        "!wget https://github.com/Sxela/ArcaneGAN/releases/download/v0.1/ArcaneGANv0.1.jit\n",
        "!wget https://github.com/Sxela/ArcaneGAN/releases/download/v0.2/ArcaneGANv0.2.jit\n",
        "!wget https://github.com/Sxela/ArcaneGAN/releases/download/v0.3/ArcaneGANv0.3.jit\n",
        "!wget https://github.com/Sxela/ArcaneGAN/releases/download/v0.4/ArcaneGANv0.4.jit\n",
        "!pip -qq install facenet_pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm7x7XgxUUwv"
      },
      "source": [
        "#@title Define functions\n",
        "#@markdown Select model version and run.\n",
        "from facenet_pytorch import MTCNN\n",
        "from torchvision import transforms\n",
        "import torch, PIL\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "mtcnn = MTCNN(image_size=256, margin=80)\n",
        "\n",
        "# simplest ye olde trustworthy MTCNN for face detection with landmarks\n",
        "def detect(img):\n",
        " \n",
        "        # Detect faces\n",
        "        batch_boxes, batch_probs, batch_points = mtcnn.detect(img, landmarks=True)\n",
        "        # Select faces\n",
        "        if not mtcnn.keep_all:\n",
        "            batch_boxes, batch_probs, batch_points = mtcnn.select_boxes(\n",
        "                batch_boxes, batch_probs, batch_points, img, method=mtcnn.selection_method\n",
        "            )\n",
        " \n",
        "        return batch_boxes, batch_points\n",
        "\n",
        "# my version of isOdd, should make a separate repo for it :D\n",
        "def makeEven(_x):\n",
        "  return _x if (_x % 2 == 0) else _x+1\n",
        "\n",
        "# the actual scaler function\n",
        "def scale(boxes, _img, max_res=1_500_000, target_face=256, fixed_ratio=0, max_upscale=2, VERBOSE=False):\n",
        " \n",
        "    x, y = _img.size\n",
        " \n",
        "    ratio = 2 #initial ratio\n",
        " \n",
        "    #scale to desired face size\n",
        "    if (boxes is not None):\n",
        "      if len(boxes)>0:\n",
        "        ratio = target_face/max(boxes[0][2:]-boxes[0][:2]); \n",
        "        ratio = min(ratio, max_upscale)\n",
        "        if VERBOSE: print('up by', ratio)\n",
        "\n",
        "    if fixed_ratio>0:\n",
        "      if VERBOSE: print('fixed ratio')\n",
        "      ratio = fixed_ratio\n",
        " \n",
        "    x*=ratio\n",
        "    y*=ratio\n",
        " \n",
        "    #downscale to fit into max res \n",
        "    res = x*y\n",
        "    if res > max_res:\n",
        "      ratio = pow(res/max_res,1/2); \n",
        "      if VERBOSE: print(ratio)\n",
        "      x=int(x/ratio)\n",
        "      y=int(y/ratio)\n",
        " \n",
        "    #make dimensions even, because usually NNs fail on uneven dimensions due skip connection size mismatch\n",
        "    x = makeEven(int(x))\n",
        "    y = makeEven(int(y))\n",
        "    \n",
        "    size = (x, y)\n",
        "\n",
        "    return _img.resize(size)\n",
        "\n",
        "\"\"\" \n",
        "    A useful scaler algorithm, based on face detection.\n",
        "    Takes PIL.Image, returns a uniformly scaled PIL.Image\n",
        "    boxes: a list of detected bboxes\n",
        "    _img: PIL.Image\n",
        "    max_res: maximum pixel area to fit into. Use to stay below the VRAM limits of your GPU.\n",
        "    target_face: desired face size. Upscale or downscale the whole image to fit the detected face into that dimension.\n",
        "    fixed_ratio: fixed scale. Ignores the face size, but doesn't ignore the max_res limit.\n",
        "    max_upscale: maximum upscale ratio. Prevents from scaling images with tiny faces to a blurry mess.\n",
        "\"\"\"\n",
        "\n",
        "def scale_by_face_size(_img, max_res=1_500_000, target_face=256, fix_ratio=0, max_upscale=2, VERBOSE=False):\n",
        "    boxes = None\n",
        "    boxes, _ = detect(_img)\n",
        "    if VERBOSE: print('boxes',boxes)\n",
        "    img_resized = scale(boxes, _img, max_res, target_face, fix_ratio, max_upscale, VERBOSE)\n",
        "    return img_resized\n",
        "\n",
        "\n",
        "size = 256\n",
        "\n",
        "means = [0.485, 0.456, 0.406]\n",
        "stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "t_stds = torch.tensor(stds).cuda().half()[:,None,None]\n",
        "t_means = torch.tensor(means).cuda().half()[:,None,None]\n",
        "\n",
        "def makeEven(_x):\n",
        "  return int(_x) if (_x % 2 == 0) else int(_x+1)\n",
        "\n",
        "img_transforms = transforms.Compose([                        \n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(means,stds)])\n",
        " \n",
        "def tensor2im(var):\n",
        "     return var.mul(t_stds).add(t_means).mul(255.).clamp(0,255).permute(1,2,0)\n",
        "\n",
        "def proc_pil_img(input_image, model):\n",
        "    transformed_image = img_transforms(input_image)[None,...].cuda().half()\n",
        "            \n",
        "    with torch.no_grad():\n",
        "        result_image = model(transformed_image)[0]; print(result_image.shape)\n",
        "        output_image = tensor2im(result_image)\n",
        "        output_image = output_image.detach().cpu().numpy().astype('uint8')\n",
        "        output_image = PIL.Image.fromarray(output_image)\n",
        "    return output_image\n",
        "\n",
        "#load model\n",
        "\n",
        "version = '0.4' #@param ['0.1','0.2','0.3','0.4']\n",
        "\n",
        "model_path = f'/content/ArcaneGANv{version}.jit' \n",
        "in_dir = '/content/in'\n",
        "out_dir = f\"/content/{model_path.split('/')[-1][:-4]}_out\"\n",
        "\n",
        "model = torch.jit.load(model_path).eval().cuda().half()\n",
        "\n",
        "#setup colab interface\n",
        "\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output \n",
        "from IPython.display import display\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "def reset(p):\n",
        "  with output_reset:\n",
        "    clear_output()\n",
        "  clear_output()\n",
        "  process()\n",
        " \n",
        "button_reset = widgets.Button(description=\"Upload\")\n",
        "output_reset = widgets.Output()\n",
        "button_reset.on_click(reset)\n",
        "\n",
        "def fit(img,maxsize=512):\n",
        "  maxdim = max(*img.size)\n",
        "  if maxdim>maxsize:\n",
        "    ratio = maxsize/maxdim\n",
        "    x,y = img.size\n",
        "    size = (int(x*ratio),int(y*ratio)) \n",
        "    img = img.resize(size)\n",
        "  return img\n",
        " \n",
        "def show_img(f, size=1024):\n",
        "  display(fit(PIL.Image.open(f),size))\n",
        "\n",
        "def process(upload=True):\n",
        "  os.makedirs(in_dir, exist_ok=True)\n",
        "  %cd {in_dir}/\n",
        "  !rm -rf {out_dir}/*\n",
        "  os.makedirs(out_dir, exist_ok=True)\n",
        "  in_files = sorted(glob(f'{in_dir}/*'))\n",
        "  if (len(in_files)==0) | (upload):\n",
        "    !rm -rf {in_dir}/*\n",
        "    uploaded = files.upload()\n",
        "    if len(uploaded.keys())<=0: \n",
        "      print('\\nNo files were uploaded. Try again..\\n')\n",
        "      return\n",
        "\n",
        "  print('\\nPress the button and pick some photos to upload\\n')\n",
        "  \n",
        "  in_files = sorted(glob(f'{in_dir}/*'))\n",
        "  for img in tqdm(in_files):\n",
        "    out = f\"{out_dir}/{img.split('/')[-1].split('.')[0]}.jpg\"\n",
        "    im = PIL.Image.open(img).convert(\"RGB\") \n",
        "    im = scale_by_face_size(im, target_face=300, max_res=1_500_000, max_upscale=2)\n",
        "    res = proc_pil_img(im, model)\n",
        "    res.save(out)\n",
        "\n",
        "  out_zip = f\"{out_dir}.zip\"\n",
        "  !zip {out_zip} {out_dir}/*\n",
        "    \n",
        "  processed = sorted(glob(f'{out_dir}/*'))[:3]\n",
        "  for f in processed: \n",
        "    show_img(f, 256)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdePnlXFX7x8",
        "cellView": "form"
      },
      "source": [
        "#@title Click to upload files and run inference. Results will be saved and zipped.\n",
        "process()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}