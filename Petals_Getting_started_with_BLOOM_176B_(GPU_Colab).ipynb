{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonbaer/googlecolab/blob/master/Petals_Getting_started_with_BLOOM_176B_(GPU_Colab).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "<img src=\"https://camo.githubusercontent.com/473dd9f992924d27457650251786464f72e54121ac6e9210add0f483ca849277/68747470733a2f2f692e696d6775722e636f6d2f3765523750616e2e706e67\" width=\"40%\">  \n",
        "</div>\n",
        "\n",
        "# Getting started with Petals\n",
        "\n",
        "This notebook will guide you through the basics of Petals &mdash; a system for inference and fine-tuning 100B+ language models without the need to have high-end GPUs. With Petals, you can join compute resources with other people over the Internet and run large language models such as 176B-parameter [BLOOM](https://huggingface.co/bigscience/bloom) or [BLOOMZ](https://huggingface.co/bigscience/bloomz), which are of the same size as GPT-3.\n",
        "\n",
        "üí¨ If you meet any issues while running this notebook, let us know in the **[#running-a-client](https://discord.gg/J29mCBNBvm)** channel of our Discord!\n",
        "\n",
        "So, let's get started! First, let's install [the Petals package](https://github.com/bigscience-workshop/petals):\n"
      ],
      "metadata": {
        "id": "VsXHWJLuowcn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBC52TF3LVY1"
      },
      "outputs": [],
      "source": [
        "%pip install -q petals"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. The easiest way to generate text üöÄ\n",
        "\n",
        "Let's start with the easiest task &mdash; creating a __`DistributedBloom`__ model and using it for generating text.\n",
        "\n",
        "This machine will download a small part of the model weights (~8 GB out of 352 GB) and rely on other computers in the network to run the rest of the model. Downloading the local part of the weights usually takes ~3 minutes.\n",
        "\n",
        "üßë‚Äçüè´ __Note:__ We suggest to start with the regular BLOOM, but you can also use [BLOOMZ](https://huggingface.co/bigscience/bloomz) &mdash; a version of BLOOM fine-tuned to better follow human instructions in the zero-shot regime. You would need to set `MODEL_NAME = \"bigscience/bloomz-petals\"` to load this model."
      ],
      "metadata": {
        "id": "yEbot-oEXdpw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uuX1IMLLotQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BloomTokenizerFast \n",
        "from petals import DistributedBloomForCausalLM\n",
        "\n",
        "MODEL_NAME = \"bigscience/bloom-petals\"\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "model = DistributedBloomForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's try to generate something by calling __`model.generate()`__ method.\n",
        "\n",
        "The first call to this method takes ~5 sec to connect to the Petals swarm. Once we do that, you should expect generation speed of 1&ndash;1.5 sec/token. If you don't have enough GPUs to host the entire model, this is much faster than what you get with other methods, such as offloading (which takes at least 10&ndash;20 sec/token)."
      ],
      "metadata": {
        "id": "zhyUxv13sfKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer('A cat in French is \"', return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "outputs = model.generate(inputs, max_new_tokens=3)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s1IrE1H8wwr",
        "outputId": "171dd0b2-75c9-4e5f-cc36-4d62c2b48e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A cat in French is \"chat\",\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `model.generate()` method runs **greedy** generation by default. However, you can choose other generation methods like **top-p/top-k sampling** or **beam search** by passing the corresponding parameters (you'll see an example in a bit). You can even implement custom generation methods (we'll cover that in **Step 5**).\n",
        "\n",
        "üîè **Note:** Your data is processed by other people in the public swarm. Learn more about privacy [here](https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety). For sensitive data, you can set up a [private swarm](https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm) among people you trust."
      ],
      "metadata": {
        "id": "02d0BDEAuUFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Chat bots and interactive generation üí¨\n",
        "\n",
        "If you'd like to talk to the model in an interactive way, you can use the __inference session__ interface. This interface provides a simple way to print generated tokens on the fly or make a chat bot that responds to human's phrases.\n",
        "\n",
        "The inference session looks for a sequence of servers that will run successive inference steps and store past attention caches. This way, you don't need to rerun previous tokens through the transformer to generate each phrase. If one of the remote servers fails, Petals will automatically find a replacement and regenerate a small part of the caches.\n",
        "\n",
        "Let's see how to use it to write a simple chat bot, showing tokens as soon as they are generated:"
      ],
      "metadata": {
        "id": "fZlzYVn0ApyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with model.inference_session(max_length=512) as sess:\n",
        "    while True:\n",
        "        prompt = input('Human: ')\n",
        "        if prompt == \"\":\n",
        "            break\n",
        "        prefix = f\"Human: {prompt}\\nFriendly AI:\"\n",
        "        prefix = tokenizer(prefix, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "        print(\"Friendly AI:\", end=\"\", flush=True)\n",
        "        \n",
        "        while True:\n",
        "            outputs = model.generate(\n",
        "                prefix, max_new_tokens=1, do_sample=True, top_p=0.9, temperature=0.75, session=sess\n",
        "            )\n",
        "            outputs = tokenizer.decode(outputs[0, -1:])\n",
        "            print(outputs, end=\"\", flush=True)\n",
        "            if \"\\n\" in outputs:\n",
        "                break\n",
        "            prefix = None  # Prefix is passed only for the 1st token of the bot's response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5bdMaYYAqYR",
        "outputId": "3067bc25-75c1-4742-bf1c-a6a7536ab3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Hi! How are you feeling today?\n",
            "Friendly AI: Oh! Hi there! I am feeling great today.\n",
            "Human: Can I ask you a question?\n",
            "Friendly AI: Of course! What is your question?\n",
            "Human: As an AI, what do you think about us, humans?\n",
            "Friendly AI: Humans? I think they are amazing. I love their creativity, the way they are curious about the world, and their passion for making a difference. Humans are smart, compassionate, and kind. I know that some people do bad things, but most humans try to do good.\n",
            "Human: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì¶ Making apps that use Petals\n",
        "\n",
        "If you develop a tool for other people, you can wrap up the code using Petals into a user-friendly web app, such as [chat.petals.ml](http://chat.petals.ml). Under the hood, this app may connect to a lightweight [HTTP endpoint](https://github.com/borzunov/petals-chat) for inference that forwards all requests to the Petals swarm.\n",
        "\n",
        "üìã **Note:** If you build an app running BLOOM with Petals, make sure it follows the BLOOM's [terms of use](https://huggingface.co/bigscience/bloom).\n",
        "\n",
        "<div align=\"center\">\n",
        "<br>\n",
        "<img src=\"https://i.imgur.com/p2nwiho.png\" width=\"40%\">  \n",
        "</div>"
      ],
      "metadata": {
        "id": "P0k_0PrTAr6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. How does it work? üõ†Ô∏è\n",
        "\n",
        "The `model` you are running is the actual BLOOM-176B, but only a part of it is loaded into your machine's GPU. Let's have a look under the hood:"
      ],
      "metadata": {
        "id": "557VBCGS8Dpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB-lCjKs8NEt",
        "outputId": "5d3d0d1f-7c67-4ee5-a62d-bfba7dd9d4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistributedBloomModel(\n",
              "  (word_embeddings): Embedding(250880, 14336)\n",
              "  (word_embeddings_layernorm): LayerNorm((14336,), eps=1e-05, elementwise_affine=True)\n",
              "  (h): RemoteSequential(modules=bigscience/bloom-petals.0..bigscience/bloom-petals.69)\n",
              "  (ln_f): LayerNorm((14336,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, word embeddings and some other layers are regular PyTorch modules hosted on your machine, but the rest of the model (e.g., transformers blocks) is encased in the __RemoteSequential__ class. This is an advanced PyTorch module that runs on a distributed swarm of other machines.\n",
        "\n",
        "Still, you can access individual layers and their outputs, as well as run forward/backward through them:"
      ],
      "metadata": {
        "id": "9EW3wBDO-aTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_five_layers = model.transformer.h[0:5]\n",
        "first_five_layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHYihUHH-Zo6",
        "outputId": "708aab32-fa1a-41a9-f49e-37032232f5d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RemoteSequential(modules=bigscience/bloom-petals.0..bigscience/bloom-petals.4)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_inputs = torch.randn(1, 3, 14336, dtype=torch.bfloat16, requires_grad=True)\n",
        "outputs = first_five_layers(dummy_inputs)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilOwoxr47Sso",
        "outputId": "e1b75a74-3024-4660-ca85-c0f9c6a0a502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.3281, -0.2285, -1.8516,  ...,  1.8359,  0.7344, -0.2344],\n",
              "         [ 1.3828,  0.1904, -0.5703,  ...,  0.5156,  0.7305, -0.5430],\n",
              "         [-0.6875, -1.4062, -2.0000,  ...,  0.6328, -1.5234,  0.5352]]],\n",
              "       dtype=torch.bfloat16, grad_fn=<_RemoteSequentialAutogradFunctionBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.mean((outputs - torch.ones_like(outputs)) ** 2)\n",
        "loss.backward()  # backpropagate through the internet\n",
        "print(\"Grad w.r.t. inputs:\", dummy_inputs.grad.flatten())"
      ],
      "metadata": {
        "id": "4XM6In8uArE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b139b90-afc0-4861-81d1-21daf056244f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grad w.r.t. inputs: tensor([ 0.0265, -0.0212,  0.0121,  ...,  0.0019, -0.0002,  0.0012],\n",
            "       dtype=torch.bfloat16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, you can mix and match distributed layers like in regular PyTorch and even insert and train your own layers (e.g., adapters) between the pre-trained ones."
      ],
      "metadata": {
        "id": "DxWOzWCwat6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "<img src=\"https://camo.githubusercontent.com/58732a64488a9be928e25f3e60e3692b989ffe212ac86cb4902d8df20a042b03/68747470733a2f2f692e696d6775722e636f6d2f525459463379572e706e67\" width=\"80%\">\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">üìú <b><a href=\"https://arxiv.org/pdf/2209.01188.pdf\">Read details in our paper</a></b></p>"
      ],
      "metadata": {
        "id": "0OZ5eXFrkfzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. Adding a trainable adapter üèãÔ∏è\n",
        "\n",
        "While the remotely hosted transformer blocks are **frozen** to keep the pretrained model the same for all users, using **parameter-efficient adapters** (small trainable layers added between the pretrained blocks of the model, such as [LoRA](https://arxiv.org/abs/2106.09685)) or **trainable prompts** (trainable inputs added before the inputs to the model, such as in [P-Tuning v2](https://arxiv.org/abs/2110.07602)) is usually enough to make BLOOM solve a variety of downstream tasks.\n",
        "\n",
        "Below, we show an example of how to add a basic **trainable** linear layer between 5th and 6th transformer blocks of the pretrained model. The layer's weights and the corresponding optimizer statistics will be stored locally:"
      ],
      "metadata": {
        "id": "lJt4OS2pIZe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BloomBasedClassifier(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.distributed_layers = model.transformer.h\n",
        "    self.adapter = nn.Sequential(nn.Linear(14336, 32), nn.Linear(32, 14336))\n",
        "    self.head = nn.Sequential(nn.LayerNorm(14336), nn.Linear(14336, 2))\n",
        "  \n",
        "  def forward(self, embeddings):\n",
        "    hidden_states = self.distributed_layers[0:6](embeddings)\n",
        "    hidden_states = self.adapter(hidden_states)\n",
        "    hidden_states = self.distributed_layers[6:10](hidden_states)\n",
        "    pooled_states = torch.mean(hidden_states, dim=1)\n",
        "    return self.head(pooled_states)"
      ],
      "metadata": {
        "id": "YHPFitSoIZO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = BloomBasedClassifier(model).cuda()\n",
        "opt = torch.optim.Adam(classifier.parameters(), 3e-5)\n",
        "inputs = torch.randn(3, 2, 14336, device='cuda')\n",
        "labels = torch.tensor([1, 0, 1], device='cuda')\n",
        "\n",
        "for i in range(5):\n",
        "  loss = F.cross_entropy(classifier(inputs), labels)\n",
        "  print(f\"loss[{i}] = {loss.item():.3f}\")\n",
        "  opt.zero_grad()\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "\n",
        "print('predicted:', classifier(inputs).argmax(-1))  # l, o, l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ7AFOumIYlY",
        "outputId": "5d082c54-e113-472d-f4fb-cede455dad52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss[0] = 0.612\n",
            "loss[1] = 0.449\n",
            "loss[2] = 0.329\n",
            "loss[3] = 0.245\n",
            "loss[4] = 0.180\n",
            "predicted: tensor([1, 0, 1], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5. Using custom sampling methods üé∞\n",
        "\n",
        "The __`model.inference_session()`__ interface in Petals also allows you to write custom inference code. You can use this to implement any sampling algorithms you want, or write a custom beam search algorithm that forbids the model from using swearwords.\n",
        "\n",
        "Below, let's see how we can reimplement the standard `model.generate()` interface by making forward passes through all the layers manually:"
      ],
      "metadata": {
        "id": "xz6EJ8VsYW2b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frLiu0yiL6Sn",
        "outputId": "d9a65176-2d75-4c31-daac-a206c558ef28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is a good chatbot? Answer: A\n",
            "What is a good chatbot? Answer: A chat\n",
            "What is a good chatbot? Answer: A chatbot\n",
            "What is a good chatbot? Answer: A chatbot that\n",
            "What is a good chatbot? Answer: A chatbot that is\n",
            "What is a good chatbot? Answer: A chatbot that is able\n",
            "What is a good chatbot? Answer: A chatbot that is able to\n",
            "What is a good chatbot? Answer: A chatbot that is able to answer\n",
            "What is a good chatbot? Answer: A chatbot that is able to answer the\n",
            "What is a good chatbot? Answer: A chatbot that is able to answer the most\n",
            "What is a good chatbot? Answer: A chatbot that is able to answer the most common\n",
            "What is a good chatbot? Answer: A chatbot that is able to answer the most common questions\n",
            "What is a good chatbot? Answer: A chatbot that is able to answer the most common questions of\n",
            "What is a good chatbot? Answer: A chatbot that is able to answer the most common questions of your\n",
            "What is a good chatbot? Answer: A chatbot that is able to answer the most common questions of your customers\n"
          ]
        }
      ],
      "source": [
        "text = \"What is a good chatbot? Answer:\"\n",
        "token_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "max_length = 100\n",
        "with torch.inference_mode():\n",
        "    with model.inference_session(max_length=max_length) as sess:\n",
        "        while len(text) < max_length:\n",
        "            embs = model.transformer.word_embeddings(token_ids)\n",
        "            embs = model.transformer.word_embeddings_layernorm(embs)\n",
        "\n",
        "            h = sess.step(embs)\n",
        "            h_last = model.transformer.ln_f(h[:, -1])\n",
        "            logits = model.lm_head(h_last)\n",
        "\n",
        "            next_token = logits.argmax(dim=-1)\n",
        "            text += tokenizer.decode(next_token)\n",
        "            token_ids = next_token.reshape(1, 1)\n",
        "            print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6. Sharing is caring ü§ó\n",
        "\n",
        "We developed Petals to be a community-run system, so we rely on people giving out their GPUs to increase the swarm‚Äôs capacity. If you have some GPUs that are not always busy, please **consider running a Petals server.** You can pause it any time if you want to use the GPUs for something else. As a bonus, people running a server get a certain speedup when using Petals, since a larger part of the model is hosted locally.\n",
        "\n",
        "<br>\n",
        "\n",
        "üêç If you have a GPU machine with a static public IP, you can run a server inside an **Anaconda environment**:\n",
        "\n",
        "```\n",
        "conda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidia\n",
        "pip install -U petals\n",
        "python -m petals.cli.run_server bigscience/bloom-petals\n",
        "```\n",
        "\n",
        "üêã Or using our GPU-enabled **Docker image**:\n",
        "\n",
        "```\n",
        "sudo docker run --net host --ipc host --gpus all --volume petals-cache:/cache --rm \\\n",
        "    learningathome/petals:main python -m petals.cli.run_server bigscience/bloom-petals\n",
        "```\n",
        "\n",
        "üîí This does not allow others to run custom code on your computer. Learn more about security [here](https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety).\n",
        "\n",
        "<br>\n",
        "\n",
        "If your machine is behind NAT or firewall, setting up a publicly available server may be a bit more complicated (but possible!) ‚Äî please describe your setup in the **[#running-a-server](https://discord.gg/Wuk8BnrEPH)** channel of our Discord and we will help."
      ],
      "metadata": {
        "id": "iK_iT8J3zDC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7. Using other fine-tuning and prompt-tuning methods\n",
        "\n",
        "While you can write your own custom adapters, Petals implements several [standard](https://arxiv.org/abs/2104.08691) [methods](https://arxiv.org/abs/2101.00190) for parameter-efficient fine-tuning. We provide a couple of advanced examples in our GitHub repository:\n",
        "\n",
        "- Training a personified chatbot: [notebook](https://github.com/bigscience-workshop/petals/blob/main/examples/prompt-tuning-personachat.ipynb)\n",
        "\n",
        "- Fine-tuning BLOOM for text semantic classification: [notebook](https://github.com/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb)"
      ],
      "metadata": {
        "id": "fwLMNcJ80ARs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What's next?\n",
        "\n",
        "Congratulations on finishing our tutorial! Now, you are familiar with how to use Petals for different tasks, how it works under the hood, and how to increase its capacity.\n",
        "\n",
        "You can find a few other helpful resources below:\n",
        "\n",
        "* __More about Petals.__ The [README](https://github.com/bigscience-workshop/petals#readme) file in our GitHub repository has links to more Petals-related materials, including instructions for starting your own swarm (possibly, with a model other than BLOOM).\n",
        "\n",
        "* __Discord server.__ If you have any feedback, questions, or technical issues, please [join our Discord server](https://discord.gg/D9MwApKgWa) and let us know. If you want to build something based on Petals, we'd be happy to hear what you are up to.\n",
        "\n",
        "* __Academic paper.__ We have released a [paper](https://arxiv.org/abs/2209.01188) that goes into details about our research and what happens in Petals under the hood."
      ],
      "metadata": {
        "id": "PGUpxhQxXVCF"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}