{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonbaer/googlecolab/blob/master/Gorilla_hosted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gorilla Hosted - Try it out in less than 60s 🚀\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ShishirPatil/gorilla)  [![arXiv](https://img.shields.io/badge/arXiv-2305.15334-<COLOR>.svg?style=flat-square)](https://arxiv.org/abs/2305.15334)   [![Discord](https://img.shields.io/discord/1111172801899012102?label=Discord&logo=discord&logoColor=green&style=flat-square)](https://discord.gg/SwTyuTAxX3)  [![Twitter](https://img.shields.io/twitter/url?url=https://twitter.com/shishirpatil_/status/1661780076277678082)](https://twitter.com/shishirpatil_/status/1661780076277678082)\n",
        "\n",
        "Play around with Gorilla! Here, we host the Gorilla zero-shot models, so you can try it out! This is compatible with the OpenAI chat completion API - plug and play!\n",
        "\n",
        "🟢 Now with Apache-2.0! Gorilla is commercially usable with no obligations 🚀\n",
        "\n",
        "We are happy to launch all three models: `gorilla-7b-hf-v1` which chooses from 925 Hugging Face APIs 0-shot, `gorilla-7b-th-v0` for 94 (exhaustive) Tensor Hub APIs 0-shot, `gorilla-7b-tf-v0` for 626 (exhaustive) Tensorflow Hub APIs 0-shot. `gorilla-mpt-7b-hf-v0` and `gorilla-falcon-7b-hf-v0`are two Apache-2.0 licensed models for Hugging Face APIs. We have a hosted end-point for `gorilla-mpt-7b-hf-v0` in this colab, and are in the process of adding `gorilla-falcon-7b-hf-v0` soon! In spirit of openess, we do not filter, nor carry out any post processing either to the prompt nor response. We will release the combined {HF+TF+TH} model which also has generic chat capability slowly to accomodate server demand.\n",
        "\n",
        "💃 If you want to use Gorilla or build on top of it! Feel absolutely free to do so - we believe in open source research and you don't even have to tell us! In case you choose to do, we have a vibrant community in Discord! Stop by and say Hi 👋\n",
        "\n",
        "<img src=\"https://github.com/ShishirPatil/gorilla/blob/gh-pages/assets/img/logo.png?raw=true\" width=30% height=30%>"
      ],
      "metadata": {
        "id": "7bKku43frr8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gorilla 🦍 is hosted by UC Berkeley Sky lab for FREE 🤩 as a research prototype 🤓\n",
        "## Please don't use it for commercial serving 👀\n",
        "## The hosted models are only trained to serve HuggingFace/TF/Torch APIs. They are NOT trained to serve other restful APIs."
      ],
      "metadata": {
        "id": "5PA9GQbV4rcN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBd_fso7qFPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "886c7b49-2243-425b-96be-ee89282a75b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.8 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "# Import Chat completion template and set-up variables\n",
        "!pip install openai &> /dev/null\n",
        "import openai\n",
        "import urllib.parse\n",
        "\n",
        "openai.api_key = \"EMPTY\" # Key is ignored and does not matter\n",
        "openai.api_base = \"http://zanino.millennium.berkeley.edu:8000/v1\"\n",
        "# Alternate mirrors\n",
        "# openai.api_base = \"http://34.132.127.197:8000/v1\"\n",
        "\n",
        "# Report issues\n",
        "def raise_issue(e, model, prompt):\n",
        "    issue_title = urllib.parse.quote(\"[bug] Hosted Gorilla: <Issue>\")\n",
        "    issue_body = urllib.parse.quote(f\"Exception: {e}\\nFailed model: {model}, for prompt: {prompt}\")\n",
        "    issue_url = f\"https://github.com/ShishirPatil/gorilla/issues/new?assignees=&labels=hosted-gorilla&projects=&template=hosted-gorilla-.md&title={issue_title}&body={issue_body}\"\n",
        "    print(f\"An exception has occurred: {e} \\nPlease raise an issue here: {issue_url}\")\n",
        "\n",
        "# Query Gorilla server\n",
        "def get_gorilla_response(prompt=\"I would like to translate from English to French.\", model=\"gorilla-7b-hf-v1\"):\n",
        "  try:\n",
        "    completion = openai.ChatCompletion.create(\n",
        "      model=model,\n",
        "      messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "  except Exception as e:\n",
        "    raise_issue(e, model, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧑‍💻 [Update Jun 15] With our new v1 model `gorilla-7b-hf-delta-v1`, Gorilla now returns code snippets you can use directly in your workflow!"
      ],
      "metadata": {
        "id": "FNDBCAMBV0aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: Translation ✍ with 🤗"
      ],
      "metadata": {
        "id": "asdPNq38qIx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gorilla `gorilla-mpt-7b-hf-v1` with code snippets\n",
        "# Translation\n",
        "prompt = \"I would like to translate 'I feel very good today.' from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v1\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2lVWV9yWO0i",
        "outputId": "ac8e7aa6-9454-4906-dc57-49e9d71a1162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Natural Language Processing Translation\n",
            "<<<api_call>>>: translator = pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. Import the pipeline function from the transformers library provided by Hugging Face.\n",
            "2. Create a translation pipeline 'translation_en_to_zh' with the model 'Helsinki-NLP/opus-mt-en-zh', which is a pretrained model for English to Chinese translation.\n",
            "3. Use the created translator to translate the English text 'I feel very good today.' to Chinese.\n",
            "<<<code>>>:\n",
            "from transformers import pipeline\n",
            "\n",
            "def load_model():\n",
            "    translator = pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\n",
            "    return translator\n",
            "\n",
            "def process_data(text, translator):\n",
            "    response = translator(text)\n",
            "    translation_text = response[0]['translation_text']\n",
            "    return translation_text\n",
            "\n",
            "english_text = 'I feel very good today.'\n",
            "\n",
            "# Load the model\n",
            "translator = load_model()\n",
            "\n",
            "# Process the data\n",
            "translated_text = process_data(english_text, translator)\n",
            "print(translated_text)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2: Object detection 🔷 with 🤗"
      ],
      "metadata": {
        "id": "Gnx7YHf18DTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gorilla `gorilla-7b-hf-v1` with code snippets\n",
        "# Object Detection\n",
        "prompt = \"I want to build a robot that can detecting objects in an image ‘cat.jpeg’. Input: [‘cat.jpeg’]\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v1\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvQawbSxWoEu",
        "outputId": "cf8e992e-8a44-4561-88d5-12b9993b14b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Computer Vision Object Detection\n",
            "<<<api_call>>>: model = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. Import the necessary components from the Hugging Face Transformers library, torch, and PIL (Python Imaging Library).\n",
            "2. Open the image using PIL's Image.open() function with the provided image path.\n",
            "3. Initialize the pretrained DETR (DEtection TRansformer) model and the image processor.\n",
            "4. Generate inputs for the model using the image processor.\n",
            "5. Pass the inputs to the model, which returns object detection results.\n",
            "<<<code>>>:\n",
            "\n",
            "from transformers import AutoFeatureExtractor, AutoModelForObjectDetection\n",
            "from PIL import Image\n",
            "import torch\n",
            "\n",
            "def load_model():\n",
            "    feature_extractor = AutoFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\n",
            "    model = AutoModelForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\n",
            "    return feature_extractor, model\n",
            "\n",
            "def process_data(image_path, feature_extractor, model):\n",
            "    image = Image.open(image_path)\n",
            "    inputs = feature_extractor(images=image, return_tensors='pt')\n",
            "    outputs = model(**inputs)\n",
            "    results = feature_extractor.post_process(outputs, threshold=0.6)[0]\n",
            "    response = [model.config.id2label[label.item()] for label in results['labels']]\n",
            "    return response\n",
            "\n",
            "image_path = 'cat.jpeg'\n",
            "\n",
            "# Load the model and feature extractor\n",
            "feature_extractor, model = load_model()\n",
            "\n",
            "# Process the data\n",
            "response = process_data(image_path, feature_extractor, model)\n",
            "print(response)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's try to invoke APIs from Torch Hub instead for the same prompts!"
      ],
      "metadata": {
        "id": "Ot4EKOLXhpoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Translation ✍ with Torch Hub\n",
        "prompt = \"I would like to translate from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-th-v0\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS-ZeP0rhmzp",
        "outputId": "01e4f245-e99c-4c58-80ce-d9ad9b20f6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'domain': 'Machine Translation', 'api_call': \\\"model = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\\\", 'api_provider': 'PyTorch', 'explanation': 'Load the Transformer model from PyTorch Hub, which is specifically trained on the WMT 2014 English-French translation task.', 'code': 'import torch\\nmodel = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')'}\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⛳️ With Gorilla being fine-tuned on MPT, and Falcon, you can use Gorilla commercially with no obligations! 🟢"
      ],
      "metadata": {
        "id": "hvQ3q5AX1wqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gorilla with `gorilla-mpt-7b-hf-v0`\n",
        "prompt = \"I would like to translate from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-mpt-7b-hf-v0\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGi3wwmQ1voP",
        "outputId": "99d7af83-c705-495c-e7b1-445ae5e6c4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide the English text you would like to translate: \\\"Translate this text to Chinese:\\\"\\n<<<domain>>>: Natural Language Processing Text2Text Generation\\n<<<api_call>>>: M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>: 1. Import the necessary libraries - M2M100ForConditionalGeneration and M2M100Tokenizer from the transformers library.\\n2. Load the pretrained model 'facebook/m2m100_1.2B' and its corresponding tokenizer.\\n3. Set the source language to English (en) and use the tokenizer to tokenize the input text.\\n4. Use the model to generate the translated text in Chinese by providing the tokenized input to the 'generate' function.\\n5. Decode the generated tokens back into a readable text string using the tokenizer.\\n<<<code>>>: from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\nsrc_text = \\\"Translate this text to Chinese:\\\"\\nsrc_lang = \\\"en\\\"\\nmodel = M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\ntokenizer = M2M100Tokenizer.from_pretrained('facebook/m2m100_1.2B')\\ntokenized_input = tokenizer(src_text, return_tensors=\\\"pt\\\", src_lang=src_lang)\\ntranslated_tokens = model.generate(**tokenized_input)\\ntranslated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\\n\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We will deprecate the `gorilla-7b-hf-v0` model on July 4 when we will automatically upgrade all v0 model requests to v1. The only changes between v0 and v1 is better code snippets.\n",
        "Below are example prompt-responses for `gorilla-7b-hf-v0` Legacy Model for 🤗"
      ],
      "metadata": {
        "id": "fIRsh6Ne9f0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I would like to translate from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v0\" ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMnJzPaN5FlV",
        "outputId": "732abf5c-5c54-498c-c8ba-22d1d7161539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Natural Language Processing Text2Text Generation\n",
            "<<<api_call>>>: M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. Import the necessary libraries, which are M2M100ForConditionalGeneration and M2M100Tokenizer from the transformers package.\n",
            "2. Load the pre-trained model 'facebook/m2m100_1.2B' using the M2M100ForConditionalGeneration.from_pretrained() method. This model is designed for machine-to-machine translation tasks.\n",
            "3. Load the tokenizer using the M2M100Tokenizer.from_pretrained() method. This tokenizer is used to prepare the input text for the model and convert the translated output back into human-readable text.\n",
            "4. Define the source text for translation and tokenize it using the tokenizer.\n",
            "5. Use the model to generate the translated text using the source tokens as input.\n",
            "6. Decode the translated text using the tokenizer and print the result.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I want to build a robot that can detect objects in an image.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v0\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WochUPqf8HLa",
        "outputId": "a25c281f-d3ff-4fcd-d4ac-ff375f613484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Computer Vision Object Detection\n",
            "<<<api_call>>>: YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. We first import the necessary classes from the transformers, PIL, and requests packages. This includes YolosForObjectDetection for the object detection model and Image for processing image data.\n",
            "2. We then use the from_pretrained method of the YolosForObjectDetection class to load the pre-trained model 'hustvl/yolos-tiny'. This model has been trained for object detection tasks, which is exactly what we need for detecting objects in an image.\n",
            "3. We load the image data from a file or a URL, and then use the model to analyze the image and identify the objects within it.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚀 Using gorilla is as easy as calling `get_gorilla_response()` with your prompt! Try out Gorilla, and share your interesting findings in `#showcase` 🤩 [Discord](https://discord.gg/3apqwwME)!"
      ],
      "metadata": {
        "id": "XS5Qe6zD8tdX"
      }
    }
  ]
}