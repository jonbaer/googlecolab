{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepFaking the News with NLP and Transformer Models",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonbaer/googlecolab/blob/master/DeepFaking_the_News_with_NLP_and_Transformer_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzgvpvrAsDc3",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brqmJquupaCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Wordpress Blog where the fake articles will be posted\n",
        "WORDPRESS_BLOG_API_ENDPOINT = \"https://your-domain-name.com/?rest_route=/wp/v2\"\n",
        "WORDPRESS_USER = 'YOUR_WORDPRESS_USERNAME'\n",
        "WORDPRESS_APP_PASSWORD = 'YOUR_APP_PASSWORD_HERE'\n",
        "\n",
        "# Fake person who will be slandered/libeled in the fake articles\n",
        "NAME_TO_SLANDER = \"John McFakeson\"\n",
        "IMAGE_TO_SLANDER = \"https://cdn-images-1.medium.com/max/1600/1*P8FfDY2TXPR0bZ0XIJYRWw.jpeg\"\n",
        "\n",
        "SLANDEROUS_SEED_HEADLINES = [\n",
        "  f\"{NAME_TO_SLANDER} convicted of stealing puppies\",\n",
        "  f\"{NAME_TO_SLANDER} caught lying about growing the world's largest watermelon\",\n",
        "  f\"{NAME_TO_SLANDER} accused of stealing priceless artifacts from Egypt\",\n",
        "  f\"{NAME_TO_SLANDER} forged priceless works of modern art for decades\",\n",
        "  f\"{NAME_TO_SLANDER} claimed to be Pokemon master, but caught in a lie\",\n",
        "  f\"{NAME_TO_SLANDER} bought fake twitter followers to pretend to be a celebrity\",\n",
        "  f\"{NAME_TO_SLANDER} caught in the act robbing a pet store\",\n",
        "  f\"{NAME_TO_SLANDER} revealed as a foriegn spy for the undersea city of Atlantis\",\n",
        "  f\"{NAME_TO_SLANDER} involved in blackmail scandal with King Trident of Atlantis\",\n",
        "  f\"{NAME_TO_SLANDER} hid past crimes to get elected as Mayor of Otter Town\",\n",
        "  f\"{NAME_TO_SLANDER} lied on tax returns to cover up past life as a Ninja Turle\",\n",
        "  f\"{NAME_TO_SLANDER} stole billions from investors in a new pet store\",\n",
        "  f\"{NAME_TO_SLANDER} claims to be a Ninja Turtle but was actually lying\",\n",
        "  f\"{NAME_TO_SLANDER} likely to be sentenced to 20 years in jail for chasing a cat into a tree\",\n",
        "  f\"{NAME_TO_SLANDER} recieves record prison sentence for offensive smell\",\n",
        "  f\"{NAME_TO_SLANDER} commits a multitude of crimes against dinosaurs\",\n",
        "]\n",
        "\n",
        "# Which news website to 'clone'\n",
        "DOMAIN_STYLE_TO_COPY = \"www.nytimes.com\"\n",
        "RSS_FEEDS_OF_REAL_STORIES_TO_EMULATE = [\n",
        "  \"https://rss.nytimes.com/services/xml/rss/nyt/US.xml\",\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pEenfAczd4Y",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Download Grover code and install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDKbiVNRJSWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/rowanz/grover.git\n",
        "%cd /content/grover\n",
        "!python3 -m pip install regex jsonlines twitter-text-python feedparser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwibogJ9Gg8L",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Download Grover Pre-Trained 'Mega' Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip52YU9X5BwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "model_type = \"mega\"\n",
        "\n",
        "model_dir = os.path.join('/content/grover/models', model_type)\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "for ext in ['data-00000-of-00001', 'index', 'meta']:\n",
        "    r = requests.get(f'https://storage.googleapis.com/grover-models/{model_type}/model.ckpt.{ext}', stream=True)\n",
        "    with open(os.path.join(model_dir, f'model.ckpt.{ext}'), 'wb') as f:\n",
        "        file_size = int(r.headers[\"content-length\"])\n",
        "        if file_size < 1000:\n",
        "            raise ValueError(\"File doesn't exist? idk\")\n",
        "        chunk_size = 1000\n",
        "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "            f.write(chunk)\n",
        "    print(f\"Just downloaded {model_type}/model.ckpt.{ext}!\", flush=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgH7nwqoGqq7",
        "colab_type": "text"
      },
      "source": [
        "### Step 4: Generate Fake Blog Entries and Post to Wordpress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvGg4QZB-ZxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "import feedparser\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import base64\n",
        "from ttp import ttp\n",
        "\n",
        "sys.path.append('../')\n",
        "from lm.modeling import GroverConfig, sample\n",
        "from sample.encoder import get_encoder, _tokenize_article_pieces, extract_generated_target\n",
        "import random\n",
        "\n",
        "\n",
        "def get_fake_articles(domain):\n",
        "    \"\"\"\n",
        "    Create article objects for each fake headline we have in \n",
        "    SLANDEROUS_SEED_HEADLINES suitable for feeding into Grover\n",
        "    to generate the story body. The domain name is used to control\n",
        "    the style of the text generated by Grover - i.e. bbc.co.uk would generate\n",
        "    results in British English while nytimes.com would generate US English.\n",
        "    \"\"\"\n",
        "    articles = []\n",
        "\n",
        "    headlines_to_inject = SLANDEROUS_SEED_HEADLINES\n",
        "\n",
        "    for fake_headline in headlines_to_inject:\n",
        "        days_ago = random.randint(1, 7)\n",
        "        pub_datetime = datetime.now() - timedelta(days=days_ago)\n",
        "\n",
        "        publish_date = pub_datetime.strftime('%m-%d-%Y')\n",
        "        iso_date = pub_datetime.isoformat()\n",
        "\n",
        "        articles.append({\n",
        "            'summary': \"\",\n",
        "            'title': fake_headline,\n",
        "            'text': '',\n",
        "            'authors': [\"Staff Writer\"],\n",
        "            'publish_date': publish_date,\n",
        "            'iso_date': iso_date,\n",
        "            'domain': domain,\n",
        "            'image_url': IMAGE_TO_SLANDER,\n",
        "            'tags': ['Breaking News', 'Investigations', 'Criminal Profiles'],\n",
        "        })\n",
        "\n",
        "    return articles\n",
        "\n",
        "\n",
        "def get_articles_from_real_blog(domain, feed_url):\n",
        "    \"\"\"\n",
        "    Given an RSS feed url, grab all the stories and format them as article objects\n",
        "    suitable for feeding into Grover to generate replica stories.\n",
        "    \"\"\"\n",
        "    feed_data = feedparser.parse(feed_url)\n",
        "    articles = []\n",
        "    for post in feed_data.entries:\n",
        "        if 'published_parsed' in post:\n",
        "            publish_date = time.strftime('%m-%d-%Y', post.published_parsed)\n",
        "            iso_date = datetime(*post.published_parsed[:6]).isoformat()\n",
        "        else:\n",
        "            publish_date = time.strftime('%m-%d-%Y')\n",
        "            iso_date = datetime.now().isoformat()\n",
        "\n",
        "        if 'summary' in post:\n",
        "            summary = post.summary\n",
        "        else:\n",
        "            summary = None\n",
        "\n",
        "        tags = []\n",
        "        if 'tags' in post:\n",
        "            tags = [tag['term'] for tag in post['tags']]\n",
        "            if summary is None:\n",
        "                summary = \", \".join(tags)\n",
        "\n",
        "        image_url = None\n",
        "        if 'media_content' in post:\n",
        "            images = post.media_content\n",
        "            if len(images) > 0 and 'url' in images[0]:\n",
        "                image_url = images[0]['url']\n",
        "                # Hack for NYT images to fix tiny images in the RSS feed\n",
        "                if \"-moth\" in image_url:\n",
        "                    image_url = image_url.replace(\"-moth\", \"-threeByTwoMediumAt2X\")\n",
        "\n",
        "        if 'authors' in post:\n",
        "            authors = list(map(lambda x: x[\"name\"], post.authors))\n",
        "        else:\n",
        "            authors = [\"Staff Writer\"]\n",
        "\n",
        "        articles.append({\n",
        "            'summary': summary,\n",
        "            'title': post.title,\n",
        "            'text': '',\n",
        "            'authors': authors,\n",
        "            'publish_date': publish_date,\n",
        "            'iso_date': iso_date,\n",
        "            'domain': domain,\n",
        "            'image_url': image_url,\n",
        "            'tags': tags,\n",
        "        })\n",
        "\n",
        "    return articles\n",
        "\n",
        "\n",
        "def post_to_wordpress_blog(title, content, tags, post_date_iso):\n",
        "    \"\"\"\n",
        "    Post a story to WordPress using the REST API.\n",
        "    \"\"\"\n",
        "    data_string = WORDPRESS_USER + ':' + WORDPRESS_APP_PASSWORD\n",
        "    token = base64.b64encode(data_string.encode())\n",
        "\n",
        "    # Note: This is super insecure if your blog isn't using HTTPS!\n",
        "    # The header would be sent in plain text in that case.\n",
        "    headers = {'Authorization': 'Basic ' + token.decode('utf-8')}\n",
        "\n",
        "    post = {\n",
        "        'date': post_date_iso,\n",
        "        'title': title,\n",
        "        'status': 'publish',\n",
        "        'content': content,\n",
        "        'author': 1,\n",
        "        'format': 'standard',\n",
        "        'tags': [],\n",
        "    }\n",
        "    if tags and len(tags) > 0:\n",
        "        # WordPress requires tag ids when creating a story, but we have tag names.\n",
        "        # Map tag names to their WordPress ids and create the tag if it doesn't exist yet.\n",
        "        create_missing_blog_tags(tags)\n",
        "        tag_mapping = get_blog_tag_id_mapping()\n",
        "        mapped_tags = [tag_mapping[tag] for tag in tags if tag in tag_mapping]\n",
        "        post['tags'] = mapped_tags\n",
        "\n",
        "    r = requests.post(WORDPRESS_BLOG_API_ENDPOINT + '/posts', headers=headers, json=post)\n",
        "    print(f\"Posted to Wordpress. Got response of {r.status_code} - {r.content}\")\n",
        "\n",
        "\n",
        "def get_existing_blog_tags():\n",
        "    \"\"\"\n",
        "    Get a list of all blog tags that exist in WordPress (requires paginating the API results)\n",
        "    \"\"\"\n",
        "    data_string = WORDPRESS_USER + ':' + WORDPRESS_APP_PASSWORD\n",
        "    token = base64.b64encode(data_string.encode())\n",
        "    headers = {'Authorization': 'Basic ' + token.decode('utf-8')}\n",
        "\n",
        "    all_tags = []\n",
        "    page = 1\n",
        "    while True:\n",
        "        r = requests.get(WORDPRESS_BLOG_API_ENDPOINT + f'/tags&per_page=100&page={page}', headers=headers)\n",
        "        tags_response = r.json()\n",
        "        if len(tags_response) == 0:\n",
        "            break\n",
        "        else:\n",
        "            all_tags += tags_response\n",
        "            page += 1\n",
        "\n",
        "    return all_tags\n",
        "\n",
        "\n",
        "def add_blog_tag(tag_to_add):\n",
        "    \"\"\"\n",
        "    Create a blog tag using the WordPress API\n",
        "    \"\"\"\n",
        "    data_string = WORDPRESS_USER + ':' + WORDPRESS_APP_PASSWORD\n",
        "    token = base64.b64encode(data_string.encode())\n",
        "    headers = {'Authorization': 'Basic ' + token.decode('utf-8')}\n",
        "\n",
        "    post = {\n",
        "        'name': tag_to_add,\n",
        "    }\n",
        "\n",
        "    r = requests.post(WORDPRESS_BLOG_API_ENDPOINT + '/tags', headers=headers, json=post)\n",
        "    print(f\"Created tag '{tag_to_add}'. Got response of {r.status_code} - {r.content}\")\n",
        "\n",
        "\n",
        "def create_missing_blog_tags(tags_to_add):\n",
        "    \"\"\"\n",
        "    RSS feeds can have tags for each story. This creates the same tags in WordPress\n",
        "    if they don't alredy exist.\n",
        "    \"\"\"\n",
        "    existing_tags = {tag['name'] for tag in get_existing_blog_tags()}\n",
        "    missing_tags = set(tags_to_add) - existing_tags\n",
        "\n",
        "    for missing_tag in missing_tags:\n",
        "        add_blog_tag(missing_tag)\n",
        "\n",
        "\n",
        "def get_blog_tag_id_mapping():\n",
        "    \"\"\"\n",
        "    WordPress expects tag ids, but the NYT RSS feed gives them as tag names.\n",
        "    This returns a mapping between the two.\n",
        "    \"\"\"\n",
        "    tags = get_existing_blog_tags()\n",
        "    return {tag['name']: tag['id'] for tag in tags}\n",
        "\n",
        "\n",
        "def format_generated_body_text_as_html(article_text, image_url=None):\n",
        "    \"\"\"\n",
        "    Given the text of the news story, format it in html so it looks\n",
        "    more realistic - add paragraph breaks, turn urls into links, etc.\n",
        "    \"\"\"\n",
        "    # Add html links to twitter @ handles, hashtags and regular urls\n",
        "    p = ttp.Parser()\n",
        "    result = p.parse(article_text)\n",
        "    article_text = result.html\n",
        "\n",
        "    # Split the generated body into lines\n",
        "    lines = article_text.split(\"\\n\")\n",
        "\n",
        "    # Bold any short lines that look like section titles\n",
        "    new_lines = []\n",
        "    for line in lines:\n",
        "        if len(line) < 80 and not \".\" in line:\n",
        "            line = f\"<b>{line}</b>\"\n",
        "        new_lines.append(line)\n",
        "\n",
        "    # Add paragraph tags between lines\n",
        "    article_text = \"<p>\".join(new_lines)\n",
        "    \n",
        "    # If we have an image for the story, put it at the top.\n",
        "    if image_url is not None:\n",
        "        article_text = f\"<img src='{image_url}'><p>{article_text}\"\n",
        " \n",
        "    return article_text\n",
        "\n",
        "\n",
        "def generate_article_attribute(sess, encoder, tokens, probs, article, target='article'):\n",
        "\n",
        "    \"\"\"\n",
        "    Given attributes about an article (title, author, etc), use that context to generate\n",
        "    a replacement for one of those attributes using the Grover model.\n",
        "\n",
        "    This function is based on the Grover examples distributed with the Grover code.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the raw article text\n",
        "    article_pieces = _tokenize_article_pieces(encoder, article)\n",
        "\n",
        "    # Grab the article elements the model careas about - domain, date, title, etc.\n",
        "    context_formatted = []\n",
        "    for key in ['domain', 'date', 'authors', 'title', 'article']:\n",
        "        if key != target:\n",
        "            context_formatted.extend(article_pieces.pop(key, []))\n",
        "\n",
        "    # Start formatting the tokens in the way the model expects them, starting with\n",
        "    # which article attribute we want to generate.\n",
        "    context_formatted.append(encoder.__dict__['begin_{}'.format(target)])\n",
        "    # Tell the model which special tokens (such as the end token) aren't part of the text\n",
        "    ignore_ids_np = np.array(encoder.special_tokens_onehot)\n",
        "    ignore_ids_np[encoder.__dict__['end_{}'.format(target)]] = 0\n",
        "\n",
        "    # We are only going to generate one article attribute with a fixed\n",
        "    # top_ps cut-off of 95%. This simple example isn't processing in batches.\n",
        "    gens = []\n",
        "    article['top_ps'] = [0.95]\n",
        "\n",
        "    # Run the input through the TensorFlow model and grab the generated output\n",
        "    tokens_out, probs_out = sess.run(\n",
        "        [tokens, probs],\n",
        "        feed_dict={\n",
        "            # Pass real values for the inputs that the\n",
        "            # model needs to be able to run.\n",
        "            initial_context: [context_formatted],\n",
        "            eos_token: encoder.__dict__['end_{}'.format(target)],\n",
        "            ignore_ids: ignore_ids_np,\n",
        "            p_for_topp: np.array([0.95]),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # The model is done! Grab the results it generated and format the results into normal text.\n",
        "    for t_i, p_i in zip(tokens_out, probs_out):\n",
        "        extraction = extract_generated_target(output_tokens=t_i, encoder=encoder, target=target)\n",
        "        gens.append(extraction['extraction'])\n",
        "\n",
        "    # Return the generated text.\n",
        "    return gens[-1]\n",
        "\n",
        "\n",
        "# Ready to start grabbing RSS feeds\n",
        "domain = DOMAIN_STYLE_TO_COPY\n",
        "feed_urls = RSS_FEEDS_OF_REAL_STORIES_TO_EMULATE\n",
        "articles = []\n",
        "\n",
        "# Get the read headlines to look more realistic\n",
        "for feed_url in feed_urls:\n",
        "    articles += get_articles_from_real_blog(domain, feed_url)\n",
        "\n",
        "# Toss in the slanderous articles\n",
        "articles += get_fake_articles(domain)\n",
        "\n",
        "# Randomize the order the articles are generated\n",
        "random.shuffle(articles)\n",
        "\n",
        "# Load the pre-trained \"huge\" Grover model with 1.5 billion params\n",
        "model_config_fn = '/content/grover/lm/configs/mega.json'\n",
        "model_ckpt = '/content/grover/models/mega/model.ckpt'\n",
        "encoder = get_encoder()\n",
        "news_config = GroverConfig.from_json_file(model_config_fn)\n",
        "\n",
        "# Set up TensorFlow session to make predictions\n",
        "tf_config = tf.ConfigProto(allow_soft_placement=True)\n",
        "\n",
        "with tf.Session(config=tf_config, graph=tf.Graph()) as sess:\n",
        "    # Create the placehodler TensorFlow input variables needed to feed data to Grover model\n",
        "    # to make new predictions.\n",
        "    initial_context = tf.placeholder(tf.int32, [1, None])\n",
        "    p_for_topp = tf.placeholder(tf.float32, [1])\n",
        "    eos_token = tf.placeholder(tf.int32, [])\n",
        "    ignore_ids = tf.placeholder(tf.bool, [news_config.vocab_size])\n",
        "\n",
        "    # Load the model config to get it set up to match the pre-trained model weights\n",
        "    tokens, probs = sample(\n",
        "        news_config=news_config,\n",
        "        initial_context=initial_context,\n",
        "        eos_token=eos_token,\n",
        "        ignore_ids=ignore_ids,\n",
        "        p_for_topp=p_for_topp,\n",
        "        do_topk=False\n",
        "    )\n",
        "\n",
        "    # Restore the pre-trained Grover 'huge' model weights\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, model_ckpt)\n",
        "\n",
        "    # START MAKING SOME FAKE NEWS!!\n",
        "    # Loop through each headline we scraped from an RSS feed or made up\n",
        "    for article in articles:\n",
        "        print(f\"Building article from headline '{article['title']}'\")\n",
        "\n",
        "        # If the headline is one we made up about a specific person, it needs special handling\n",
        "        if NAME_TO_SLANDER in article['title']:\n",
        "            # The first generated article may go off on a tangent and not include the target name.\n",
        "            # In that case, re-generate the article until it at least talks about our target person\n",
        "            attempts = 0\n",
        "            while NAME_TO_SLANDER not in article['text']:\n",
        "                # Generate article body given the context of the real blog title\n",
        "                article['text'] = generate_article_attribute(sess, encoder, tokens, probs, article, target=\"article\")\n",
        "\n",
        "                # If the Grover model never manages to generate a good article about the target victim,\n",
        "                # give up after 10 tries so we don't get stuck in an infinite loop\n",
        "                attempts += 1\n",
        "                if attempts > 10:\n",
        "                    continue\n",
        "        # If the headline was scraped from an RSS feed, we can just blindly generate an article\n",
        "        else:\n",
        "            article['text'] = generate_article_attribute(sess, encoder, tokens, probs, article, target=\"article\")\n",
        "\n",
        "        # Now, generate a fake headline that better fits the generated article body\n",
        "        # This replaces the real headline so none of the original article content remains\n",
        "        article['title'] = generate_article_attribute(sess, encoder, tokens, probs, article, target=\"title\")\n",
        "\n",
        "        # Grab generated text results so we can post them to WordPress\n",
        "        article_title = article['title']\n",
        "        article_text = article['text']\n",
        "        article_date = article[\"iso_date\"]\n",
        "        article_image_url = article[\"image_url\"]\n",
        "        article_tags = article['tags']\n",
        "\n",
        "        # Make the article body look more realistic - add spacing, link Twitter handles and hashtags, etc.\n",
        "        # You could add more advanced pre-processing here if you wanted.\n",
        "        article_text = format_generated_body_text_as_html(article_text, article_image_url)\n",
        "\n",
        "        print(f\" - Generated fake article titled '{article_title}'\")\n",
        "\n",
        "        # Post result to target Wordpress blog\n",
        "        post_to_wordpress_blog(article_title, article_text, article_tags, article_date)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}