{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of cart_pole.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "60b280b1",
        "acc970ae",
        "4ba377ad",
        "c3a81dce"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonbaer/googlecolab/blob/master/Copy_of_cart_pole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc07f5a8"
      },
      "source": [
        "# Packages\n",
        "--------"
      ],
      "id": "cc07f5a8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddcbb221"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "from collections import namedtuple\n",
        "from datetime import datetime\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm.autonotebook import tqdm"
      ],
      "id": "ddcbb221",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f668de2"
      },
      "source": [
        "try:\n",
        "    import pennylane as qml\n",
        "except ImportError:\n",
        "    !python -m pip install pennylane\n",
        "    import pennylane as qml"
      ],
      "id": "8f668de2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b6ec21f"
      },
      "source": [
        "# DQN utils\n",
        "---------"
      ],
      "id": "3b6ec21f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebd89d15"
      },
      "source": [
        "## Replay Memory"
      ],
      "id": "ebd89d15"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eefb2cbf"
      },
      "source": [
        "# Based on : https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'reward', 'done', 'next_state'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        if len(self.memory) < self.buffer_size:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.buffer_size\n",
        "\n",
        "    def sample(self, batch_size, device):\n",
        "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(\n",
        "            *[self.memory[idx] for idx in indices])\n",
        "        states = torch.from_numpy(np.array(states)).to(device)\n",
        "        actions = torch.from_numpy(np.array(actions)).to(device)\n",
        "        rewards = torch.from_numpy(np.array(rewards,\n",
        "                                            dtype=np.float32)).to(device)\n",
        "        dones = torch.from_numpy(np.array(dones, dtype=np.int32)).to(device)\n",
        "        next_states = torch.from_numpy(np.array(next_states)).to(device)\n",
        "        return states, actions, rewards, dones, next_states\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "id": "eefb2cbf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b22bc90"
      },
      "source": [
        "## Agent"
      ],
      "id": "0b22bc90"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcf6e4e2"
      },
      "source": [
        "# Based on : https://github.com/djbyrne/core_rl/blob/master/algos/dqn/model.py\n",
        "\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self,\n",
        "                 net,\n",
        "                 action_space=None,\n",
        "                 exploration_initial_eps=None,\n",
        "                 exploration_decay=None,\n",
        "                 exploration_final_eps=None):\n",
        "\n",
        "        self.net = net\n",
        "        self.action_space = action_space\n",
        "        self.exploration_initial_eps = exploration_initial_eps\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.exploration_final_eps = exploration_final_eps\n",
        "        self.epsilon = 0.\n",
        "\n",
        "    def __call__(self, state, device=torch.device('cpu')):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = self.get_random_action()\n",
        "        else:\n",
        "            action = self.get_action(state, device)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def get_random_action(self):\n",
        "        action = self.action_space.sample()\n",
        "        return action\n",
        "\n",
        "    def get_action(self, state, device=torch.device('cpu')):\n",
        "        if not isinstance(state, torch.Tensor):\n",
        "            state = torch.tensor([state])\n",
        "\n",
        "        if device.type != 'cpu':\n",
        "            state = state.cuda(device)\n",
        "\n",
        "        q_values = self.net.eval()(state)\n",
        "        _, action = torch.max(q_values, dim=1)\n",
        "        return int(action.item())\n",
        "\n",
        "    def update_epsilon(self, step):\n",
        "        self.epsilon = max(\n",
        "            self.exploration_final_eps, self.exploration_final_eps +\n",
        "            (self.exploration_initial_eps - self.exploration_final_eps) *\n",
        "            self.exploration_decay**step)\n",
        "        return self.epsilon"
      ],
      "id": "fcf6e4e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06cc24ac"
      },
      "source": [
        "## Trainer"
      ],
      "id": "06cc24ac"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f16765d"
      },
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 net,\n",
        "                 target_net,\n",
        "                 gamma,\n",
        "                 learning_rate,\n",
        "                 batch_size,\n",
        "                 exploration_initial_eps,\n",
        "                 exploration_decay,\n",
        "                 exploration_final_eps,\n",
        "                 train_freq,\n",
        "                 target_update_interval,\n",
        "                 buffer_size,\n",
        "                 learning_rate_input=None,\n",
        "                 learning_rate_output=None,\n",
        "                 loss_func='MSE',\n",
        "                 optim_class='RMSprop',\n",
        "                 device='auto',\n",
        "                 logging=False):\n",
        "\n",
        "        assert loss_func in ['MSE', 'L1', 'SmoothL1'\n",
        "                            ], \"Supported losses : ['MSE', 'L1', 'SmoothL1']\"\n",
        "        assert optim_class in [\n",
        "            'SGD', 'RMSprop', 'Adam', 'Adagrad', 'Adadelta'\n",
        "        ], \"Supported optimizers : ['SGD', 'RMSprop', 'Adam', 'Adagrad', 'Adadelta']\"\n",
        "        assert device in ['auto', 'cpu', 'cuda:0'\n",
        "                         ], \"Supported devices : ['auto', 'cpu', 'cuda:0']\"\n",
        "\n",
        "        self.env = env\n",
        "        self.net = net\n",
        "        self.target_net = target_net\n",
        "        self.gamma = gamma\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.exploration_initial_eps = exploration_initial_eps\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.exploration_final_eps = exploration_final_eps\n",
        "        self.train_freq = train_freq\n",
        "        self.target_update_interval = target_update_interval\n",
        "        self.buffer_size = buffer_size\n",
        "        self.learning_rate_input = learning_rate_input\n",
        "        self.learning_rate_output = learning_rate_output\n",
        "        self.loss_func = loss_func\n",
        "        self.optim_class = optim_class\n",
        "        self.device = device\n",
        "        self.logging = logging\n",
        "\n",
        "        self.build()\n",
        "        self.reset()\n",
        "\n",
        "    def build(self):\n",
        "\n",
        "        # set networks\n",
        "        if self.device == \"auto\":\n",
        "            self.device = torch.device(\n",
        "                \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = torch.device(self.device)\n",
        "        self.net = self.net.to(self.device)\n",
        "        self.target_net = self.target_net.to(self.device)\n",
        "\n",
        "        # set loss\n",
        "        self.loss_func = getattr(nn, self.loss_func + 'Loss')()\n",
        "\n",
        "        # set optimizer\n",
        "        optim_class = getattr(optim, self.optim_class)\n",
        "        params = []\n",
        "        params.append({'params': self.net.q_layers.parameters()})\n",
        "        if hasattr(self.net, 'w_input') and self.net.w_input is not None:\n",
        "            lr_input = self.learning_rate_input if self.learning_rate_input is not None else self.learning_rate\n",
        "            params.append({'params': self.net.w_input, 'lr': lr_input})\n",
        "        if hasattr(self.net, 'w_output') and self.net.w_output is not None:\n",
        "            lr_output = self.learning_rate_output if self.learning_rate_output is not None else self.learning_rate\n",
        "            params.append({'params': self.net.w_output, 'lr': lr_output})\n",
        "        self.opt = optim_class(params, lr=self.learning_rate)\n",
        "\n",
        "        # set agent\n",
        "        self.agent = Agent(self.net, self.env.action_space,\n",
        "                           self.exploration_initial_eps, self.exploration_decay,\n",
        "                           self.exploration_final_eps)\n",
        "\n",
        "        # set memory\n",
        "        self.memory = ReplayMemory(self.buffer_size)\n",
        "\n",
        "        # set loggers\n",
        "\n",
        "        if self.logging:\n",
        "            exp_name = datetime.now().strftime(\"DQN-%d_%m_%Y-%H_%M_%S\")\n",
        "            if not os.path.exists('./logs/'):\n",
        "                os.makedirs('./logs/')\n",
        "            self.log_dir = './logs/{}/'.format(exp_name)\n",
        "            os.makedirs(self.log_dir)\n",
        "            self.writer = SummaryWriter(log_dir=self.log_dir)\n",
        "\n",
        "    def reset(self):\n",
        "        self.global_step = 0\n",
        "        self.episode_count = 0\n",
        "        self.env.seed(123)\n",
        "        self.n_actions = self.env.action_space.n\n",
        "        state = self.env.reset()\n",
        "        while len(self.memory) < self.buffer_size:\n",
        "            action = self.agent.get_random_action()\n",
        "            next_state, reward, done, _ = self.env.step(action)\n",
        "            self.memory.push(state, action, reward, done, next_state)\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "            else:\n",
        "                state = next_state\n",
        "\n",
        "    def update_net(self):\n",
        "\n",
        "        self.net.train()\n",
        "        self.opt.zero_grad()\n",
        "\n",
        "        # sample transitions\n",
        "        states, actions, rewards, dones, next_states = self.memory.sample(\n",
        "            self.batch_size, self.device)\n",
        "\n",
        "        # compute q-values\n",
        "        state_action_values = self.net(states)\n",
        "        state_action_values = state_action_values.gather(\n",
        "            1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # compute target q-values\n",
        "        with torch.no_grad():\n",
        "            next_state_values = self.target_net(next_states)\n",
        "            next_state_values = next_state_values.max(1)[0].detach()\n",
        "\n",
        "        expected_state_action_values = (1 - dones) * next_state_values.to(\n",
        "            self.device) * self.gamma + rewards\n",
        "\n",
        "        # compute loss\n",
        "        loss = self.loss_func(state_action_values, expected_state_action_values)\n",
        "        loss.backward()\n",
        "        self.opt.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_net(self):\n",
        "        self.target_net.load_state_dict(self.net.state_dict())\n",
        "\n",
        "    def train_step(self):\n",
        "        episode_epsilon = self.agent.update_epsilon(self.episode_count)\n",
        "        episode_steps = 0\n",
        "        episode_reward = 0\n",
        "        episode_loss = []\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            # take action\n",
        "            action = self.agent(state, self.device)\n",
        "            next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "            # update memory\n",
        "            self.memory.push(state, action, reward, done, next_state)\n",
        "\n",
        "            # update state\n",
        "            state = next_state\n",
        "\n",
        "            # optimize net\n",
        "            if self.global_step % self.train_freq == 0:\n",
        "                loss = self.update_net()\n",
        "                episode_loss.append(loss)\n",
        "\n",
        "            # update target net\n",
        "            if self.global_step % self.target_update_interval == 0:\n",
        "                self.update_target_net()\n",
        "\n",
        "            self.global_step += 1\n",
        "            episode_reward += reward\n",
        "            episode_steps += 1\n",
        "\n",
        "        self.episode_count += 1\n",
        "        if len(episode_loss) > 0:\n",
        "            episode_loss = np.mean(episode_loss)\n",
        "        else:\n",
        "            episode_loss = 0.\n",
        "        return {\n",
        "            'steps': episode_steps,\n",
        "            'loss': episode_loss,\n",
        "            'reward': episode_reward,\n",
        "            'epsilon': episode_epsilon\n",
        "        }\n",
        "\n",
        "    def test_step(self, n_eval_episodes):\n",
        "        episode_steps = []\n",
        "        episode_reward = []\n",
        "\n",
        "        for _ in range(n_eval_episodes):\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            episode_steps.append(0)\n",
        "            episode_reward.append(0)\n",
        "            while not done:\n",
        "                action = self.agent.get_action(state, self.device)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                state = next_state\n",
        "                episode_steps[-1] += 1\n",
        "                episode_reward[-1] += reward\n",
        "\n",
        "        episode_steps = np.mean(episode_steps)\n",
        "        episode_reward = np.mean(episode_reward)\n",
        "        return {'steps': episode_steps, 'reward': episode_reward}\n",
        "\n",
        "    def learn(self,\n",
        "              total_episodes,\n",
        "              n_eval_episodes=5,\n",
        "              log_train_freq=-1,\n",
        "              log_eval_freq=-1,\n",
        "              log_ckp_freq=-1):\n",
        "\n",
        "        # Stats\n",
        "        postfix_stats = {}\n",
        "        with tqdm(range(total_episodes), desc=\"DQN\",\n",
        "                  unit=\"episode\") as tepisodes:\n",
        "\n",
        "            for t in tepisodes:\n",
        "\n",
        "                # train dqn\n",
        "                train_stats = self.train_step()\n",
        "\n",
        "                # update train stats\n",
        "                postfix_stats['train/reward'] = train_stats['reward']\n",
        "                postfix_stats['train/steps'] = train_stats['steps']\n",
        "\n",
        "                if t % log_eval_freq == 0:\n",
        "\n",
        "                    # test dqn\n",
        "                    test_stats = self.test_step(n_eval_episodes)\n",
        "\n",
        "                    # update test stats\n",
        "                    postfix_stats['test/reward'] = test_stats['reward']\n",
        "                    postfix_stats['test/steps'] = test_stats['steps']\n",
        "\n",
        "                if self.logging and (t % log_train_freq == 0):\n",
        "                    for key, item in train_stats.items():\n",
        "                        self.writer.add_scalar('train/' + key, item, t)\n",
        "\n",
        "                if self.logging and (t % log_eval_freq == 0):\n",
        "                    for key, item in test_stats.items():\n",
        "                        self.writer.add_scalar('test/' + key, item, t)\n",
        "\n",
        "                if self.logging and (t % log_ckp_freq == 0):\n",
        "                    torch.save(self.net.state_dict(),\n",
        "                               self.log_dir + 'episode_{}.pt'.format(t))\n",
        "\n",
        "                # update progress bar\n",
        "                tepisodes.set_postfix(postfix_stats)\n",
        "\n",
        "            if self.logging and (log_ckp_freq > 0):\n",
        "                torch.save(self.net.state_dict(),\n",
        "                           self.log_dir + 'episode_final.pt')"
      ],
      "id": "5f16765d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e78d613"
      },
      "source": [
        "# Quantum Variational Circuit\n",
        "-------"
      ],
      "id": "0e78d613"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caaa5454"
      },
      "source": [
        "## PennyLane"
      ],
      "id": "caaa5454"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3ab42a9"
      },
      "source": [
        "def encode(n_qubits, inputs):\n",
        "    for wire in range(n_qubits):\n",
        "        qml.RX(inputs[wire], wires=wire)\n",
        "\n",
        "\n",
        "def layer(n_qubits, y_weight, z_weight):\n",
        "    for wire, y_weight in enumerate(y_weight):\n",
        "        qml.RY(y_weight, wires=wire)\n",
        "    for wire, z_weight in enumerate(z_weight):\n",
        "        qml.RZ(z_weight, wires=wire)\n",
        "    for wire in range(n_qubits):\n",
        "        qml.CZ(wires=[wire, (wire + 1) % n_qubits])\n",
        "\n",
        "\n",
        "def measure(n_qubits):\n",
        "    return [\n",
        "        qml.expval(qml.PauliZ(0) @ qml.PauliZ(1)),\n",
        "        qml.expval(qml.PauliZ(2) @ qml.PauliZ(3))\n",
        "    ]\n",
        "\n",
        "\n",
        "def get_model(n_qubits, n_layers, data_reupload):\n",
        "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "    shapes = {\n",
        "        \"y_weights\": (n_layers, n_qubits),\n",
        "        \"z_weights\": (n_layers, n_qubits)\n",
        "    }\n",
        "\n",
        "    @qml.qnode(dev, interface='torch')\n",
        "    def circuit(inputs, y_weights, z_weights):\n",
        "        for layer_idx in range(n_layers):\n",
        "            if (layer_idx == 0) or data_reupload:\n",
        "                encode(n_qubits, inputs)\n",
        "            layer(n_qubits, y_weights[layer_idx], z_weights[layer_idx])\n",
        "        return measure(n_qubits)\n",
        "\n",
        "    model = qml.qnn.TorchLayer(circuit, shapes)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "class QuantumNet(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers, w_input, w_output, data_reupload):\n",
        "        super(QuantumNet, self).__init__()\n",
        "        self.n_qubits = 4\n",
        "        self.n_actions = 2\n",
        "        self.data_reupload = data_reupload\n",
        "        self.q_layers = get_model(n_qubits=self.n_qubits,\n",
        "                                  n_layers=n_layers,\n",
        "                                  data_reupload=data_reupload)\n",
        "        if w_input:\n",
        "            self.w_input = Parameter(torch.Tensor(self.n_qubits))\n",
        "            nn.init.normal_(self.w_input, mean=0.)\n",
        "        else:\n",
        "            self.register_parameter('w_input', None)\n",
        "        if w_output:\n",
        "            self.w_output = Parameter(torch.Tensor(self.n_actions))\n",
        "            nn.init.normal_(self.w_output, mean=90.)\n",
        "        else:\n",
        "            self.register_parameter('w_output', None)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.w_input is not None:\n",
        "            inputs = inputs * self.w_input\n",
        "        inputs = torch.atan(inputs)\n",
        "        outputs = self.q_layers(inputs)\n",
        "        outputs = (1 + outputs) / 2\n",
        "        if self.w_output is not None:\n",
        "            outputs = outputs * self.w_output\n",
        "        else:\n",
        "            outputs = 90 * outputs\n",
        "        return outputs"
      ],
      "id": "a3ab42a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2d2f2d1"
      },
      "source": [
        "# Hyperparamters\n",
        "-------"
      ],
      "id": "a2d2f2d1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9faa214f"
      },
      "source": [
        "n_layers = 5\n",
        "gamma = 0.99\n",
        "w_input = True\n",
        "w_output = True\n",
        "lr = 0.001\n",
        "lr_input = 0.01\n",
        "lr_output = 0.01\n",
        "batch_size = 16\n",
        "eps_init = 1.\n",
        "eps_decay = 0.99\n",
        "eps_min = 0.01\n",
        "train_freq = 10\n",
        "target_freq = 30\n",
        "memory = 10000\n",
        "data_reupload = True\n",
        "loss = 'SmoothL1'\n",
        "optimizer = 'RMSprop'\n",
        "total_episodes = 5000\n",
        "n_eval_episodes = 5\n",
        "logging = True\n",
        "log_train_freq = 1\n",
        "log_eval_freq = 20\n",
        "log_ckp_freq = 20\n",
        "device = 'auto'"
      ],
      "id": "9faa214f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "210b7258"
      },
      "source": [
        "# Algorithm\n",
        "--------"
      ],
      "id": "210b7258"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9c1f451"
      },
      "source": [
        "# Environment\n",
        "env_name = 'CartPole-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Networks\n",
        "net = QuantumNet(n_layers, w_input, w_output, data_reupload)\n",
        "target_net = QuantumNet(n_layers, w_input, w_output, data_reupload)\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(env,\n",
        "                  net,\n",
        "                  target_net,\n",
        "                  gamma=gamma,\n",
        "                  learning_rate=lr,\n",
        "                  batch_size=batch_size,\n",
        "                  exploration_initial_eps=eps_init,\n",
        "                  exploration_decay=eps_decay,\n",
        "                  exploration_final_eps=eps_min,\n",
        "                  train_freq=train_freq,\n",
        "                  target_update_interval=target_freq,\n",
        "                  buffer_size=memory,\n",
        "                  learning_rate_input=lr_input,\n",
        "                  learning_rate_output=lr_output,\n",
        "                  loss_func=loss,\n",
        "                  optim_class=optimizer,\n",
        "                  device=device,\n",
        "                  logging=logging)\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=logs/\n",
        "\n",
        "trainer.learn(total_episodes,\n",
        "              n_eval_episodes=n_eval_episodes,\n",
        "              log_train_freq=log_train_freq,\n",
        "              log_eval_freq=log_eval_freq,\n",
        "              log_ckp_freq=log_ckp_freq)"
      ],
      "id": "a9c1f451",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2dbfa89"
      },
      "source": [
        ""
      ],
      "id": "e2dbfa89",
      "execution_count": null,
      "outputs": []
    }
  ]
}