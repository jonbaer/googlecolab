{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonbaer/googlecolab/blob/master/Dreambooth_on_Blueprint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "5JnpPhkPwsfd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXA7Y_uyEI9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e22b96c-b735-41c7-bc4c-b8ff62bbd71c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting baseten\n",
            "  Downloading baseten-0.5.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.9/dist-packages (from baseten) (3.1.2)\n",
            "Collecting requests-toolbelt<0.10.0,>=0.9.1\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting truss<0.5.0,>=0.4.0\n",
            "  Downloading truss-0.4.0-py3-none-any.whl (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.4/174.4 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-pyyaml<7.0.0.0,>=6.0.12.2\n",
            "  Downloading types_PyYAML-6.0.12.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.0.1 in /usr/local/lib/python3.9/dist-packages (from baseten) (8.2.2)\n",
            "Collecting semantic-version<3.0.0,>=2.10.0\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests>=2.22 in /usr/local/lib/python3.9/dist-packages (from baseten) (2.25.1)\n",
            "Collecting colorama>=0.4.3\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting Pillow<10.0.0,>=9.3.0\n",
            "  Downloading Pillow-9.4.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-pillow<10.0.0.0,>=9.3.0.4\n",
            "  Downloading types_Pillow-9.4.0.17-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coolname>=1.1.0\n",
            "  Downloading coolname-2.2.0-py2.py3-none-any.whl (37 kB)\n",
            "Collecting types-pytz<2023.0.0.0,>=2022.7.1.0\n",
            "  Downloading types_pytz-2022.7.1.2-py3-none-any.whl (4.7 kB)\n",
            "Collecting halo<0.0.32,>=0.0.31\n",
            "  Downloading halo-0.0.31.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from baseten) (4.65.0)\n",
            "Collecting types-setuptools<66.0.0.0,>=65.6.0.2\n",
            "  Downloading types_setuptools-65.7.0.4-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting single-source<0.4.0,>=0.3.0\n",
            "  Downloading single_source-0.3.0-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from baseten) (6.0)\n",
            "Requirement already satisfied: pytz<2023.0.0,>=2022.7.1 in /usr/local/lib/python3.9/dist-packages (from baseten) (2022.7.1)\n",
            "Collecting types-requests<3.0.0.0,>=2.28.11.7\n",
            "  Downloading types_requests-2.28.11.15-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from baseten) (8.1.3)\n",
            "Requirement already satisfied: joblib>=0.12.5 in /usr/local/lib/python3.9/dist-packages (from baseten) (1.1.1)\n",
            "Collecting log_symbols>=0.0.14\n",
            "  Downloading log_symbols-0.0.14-py3-none-any.whl (3.1 kB)\n",
            "Collecting spinners>=0.0.24\n",
            "  Downloading spinners-0.0.24-py3-none-any.whl (5.5 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from halo<0.0.32,>=0.0.31->baseten) (2.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from halo<0.0.32,>=0.0.31->baseten) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2>=2.10.3->baseten) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22->baseten) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22->baseten) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22->baseten) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22->baseten) (1.26.15)\n",
            "Requirement already satisfied: cloudpickle<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from truss<0.5.0,>=0.4.0->baseten) (2.2.1)\n",
            "Collecting python-on-whales<0.47.0,>=0.46.0\n",
            "  Downloading python_on_whales-0.46.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msgpack-numpy>=0.4.7.1\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Collecting python-json-logger>=2.0.2\n",
            "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Collecting packaging<21.0,>=20.9\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from truss<0.5.0,>=0.4.0->baseten) (1.0.5)\n",
            "Collecting types-urllib3<1.27\n",
            "  Downloading types_urllib3-1.26.25.8-py3-none-any.whl (15 kB)\n",
            "Collecting types-docutils\n",
            "  Downloading types_docutils-0.19.1.6-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging<21.0,>=20.9->truss<0.5.0,>=0.4.0->baseten) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from python-on-whales<0.47.0,>=0.46.0->truss<0.5.0,>=0.4.0->baseten) (4.5.0)\n",
            "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from python-on-whales<0.47.0,>=0.46.0->truss<0.5.0,>=0.4.0->baseten) (0.7.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from python-on-whales<0.47.0,>=0.46.0->truss<0.5.0,>=0.4.0->baseten) (1.10.6)\n",
            "Building wheels for collected packages: halo\n",
            "  Building wheel for halo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for halo: filename=halo-0.0.31-py3-none-any.whl size=11259 sha256=472b2f2ce0962b6cfeca9a2030497dd8955fa9a3b86c276e00bdff8f7df2ef36\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/85/47/b7c7338ab52808105f937bd8c04aec5d98a543311ac2c8bed2\n",
            "Successfully built halo\n",
            "Installing collected packages: types-urllib3, types-pyyaml, types-pytz, types-pillow, types-docutils, spinners, coolname, types-setuptools, types-requests, single-source, semantic-version, python-json-logger, Pillow, packaging, numpy, colorama, requests-toolbelt, python-on-whales, msgpack-numpy, log_symbols, truss, halo, baseten\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.0\n",
            "    Uninstalling packaging-23.0:\n",
            "      Successfully uninstalled packaging-23.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "statsmodels 0.13.5 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-9.4.0 baseten-0.5.2 colorama-0.4.6 coolname-2.2.0 halo-0.0.31 log_symbols-0.0.14 msgpack-numpy-0.4.8 numpy-1.23.5 packaging-20.9 python-json-logger-2.0.7 python-on-whales-0.46.0 requests-toolbelt-0.9.1 semantic-version-2.10.0 single-source-0.3.0 spinners-0.0.24 truss-0.4.0 types-docutils-0.19.1.6 types-pillow-9.4.0.17 types-pytz-2022.7.1.2 types-pyyaml-6.0.12.8 types-requests-2.28.11.15 types-setuptools-65.7.0.4 types-urllib3-1.26.25.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32mINFO\u001b[0m API key set.\n",
            "INFO:baseten:API key set.\n"
          ]
        }
      ],
      "source": [
        "# Install Baseten CLI \n",
        "! pip install baseten --upgrade\n",
        "import baseten\n",
        "# Get an API key from https://app.baseten.co/blueprint/keys\n",
        "baseten.login(\"xFSGHxk8.uMxt30FCJendRQoXvCZUpsYNd9P4i5DZ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload / setup dataset"
      ],
      "metadata": {
        "id": "_PYo0en1wnfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevant imports for Dreambooth fine-tuning\n",
        "from baseten.training import FinetuningRun, DreamboothConfig, LocalPath, PublicUrl"
      ],
      "metadata": {
        "id": "b9jLYcD4azwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your own dataset (docs here)\n",
        "# input_data = LocalPath(\"/path/to/my-dataset\")\n",
        "input_data = PublicUrl(\"https://cdn.baseten.co/docs/production/DreamboothSampleDataset.zip\")"
      ],
      "metadata": {
        "id": "1XiLL48xFuYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Dreambooth fine-tuning config"
      ],
      "metadata": {
        "id": "xoKRNuvSBr2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = DreamboothConfig(\n",
        "    instance_prompt=\"photo of olliedog\",\n",
        "    input_dataset=input_data,\n",
        "    class_prompt=\"photo of dog\",\n",
        "    train_text_encoder=False,\n",
        "    max_train_steps=1300,\n",
        "    learning_rate=2e-6,\n",
        ")"
      ],
      "metadata": {
        "id": "K70bzUV2ZrJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kick off fine-tuning"
      ],
      "metadata": {
        "id": "smVYSZiew2tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kick off your finetuning job \n",
        "fr = FinetuningRun.create(\"my-dog-model\", config)"
      ],
      "metadata": {
        "id": "jIeFyh1XZ6AP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "577fcc63-a00a-4380-9d3f-5136a072c630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32mINFO\u001b[0m Starting fine-tuning of dreambooth\n",
            "INFO:baseten.training.finetuning:Starting fine-tuning of dreambooth\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ApiError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApiError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-263edf2f2d36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Kick off your finetuning job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFinetuningRun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"my-dog-model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/baseten/training/finetuning.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(trained_model_name, fine_tuning_config, auto_deploy, verbose)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting fine-tuning of %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfine_tuning_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_truss_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         run_id = finetune_zoo_model(\n\u001b[0m\u001b[1;32m    543\u001b[0m             \u001b[0mtrained_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mfine_tuning_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_truss_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/baseten/common/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"api\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"api_key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mApiKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"api\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"api_key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_jwt_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/baseten/common/api.py\u001b[0m in \u001b[0;36mfinetune_zoo_model\u001b[0;34m(auth_token, trained_model_name, train_truss_name, encoded_variables, auto_deploy)\u001b[0m\n\u001b[1;32m    985\u001b[0m     \"\"\"\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_post_graphql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"finetune_model_zoo_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/baseten/common/api.py\u001b[0m in \u001b[0;36m_post_graphql_query\u001b[0;34m(auth_token, query_string)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"errors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mApiError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresp_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mApiError\u001b[0m: To proceed, please add a credit card.\n<Server response: {'errors': [{'message': 'To proceed, please add a credit card.', 'locations': [{'line': 3, 'column': 9}], 'path': ['finetune_model_zoo_model'], 'extensions': {'code': 'UNAUTHORIZED_ACCESS'}}], 'data': {'finetune_model_zoo_model': None}}>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stream logs from your finetuning job \n",
        "fr.stream_logs()"
      ],
      "metadata": {
        "id": "lRHDj28taRQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a76b4f4-f3cf-496c-cfb4-dd07e3cfff5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rℹ Starting to stream logs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32mINFO\u001b[0m [['--adam_beta1', '0.9'], ['--adam_beta2', '0.999'], ['--adam_epsilon', '1e-08'], ['--adam_weight_decay', '0.01'], ['--center_crop', None], ['--class_data_dir', None], ['--class_prompt', 'photo of dog'], ['--gradient_accumulation_steps', '1'], ['--hf_access_token', None], ['--image_log_steps', '20'], ['--instance_prompt', 'photo of olliedog'], ['--learning_rate', '2e-06'], ['--lr_scheduler', 'constant'], ['--lr_warmup_steps', '500'], ['--max_grad_norm', '1.0'], ['--max_train_steps', '1300'], ['--mixed_precision', 'fp16'], ['--num_class_images', '100'], ['--num_train_epochs', '1'], ['--pretrained_model_name_or_path', 'CompVis/stable-diffusion-v1-4'], ['--prior_loss_weight', '1.0'], ['--resolution', '512'], ['--revision', None], ['--sample_batch_size', '1'], ['--seed', None], ['--tokenizer_name', None], ['--train_batch_size', '1'], ['--train_text_encoder', None], ['--wandb_api_key', None], ['--with_prior_preservation', None], ['--gradient_checkpointing', 'True'], ['--instance_data_dir', '/app/DreamboothSampleDataset/object'], ['--output_dir', '/output']]\n",
            "INFO:baseten.training.logs:[['--adam_beta1', '0.9'], ['--adam_beta2', '0.999'], ['--adam_epsilon', '1e-08'], ['--adam_weight_decay', '0.01'], ['--center_crop', None], ['--class_data_dir', None], ['--class_prompt', 'photo of dog'], ['--gradient_accumulation_steps', '1'], ['--hf_access_token', None], ['--image_log_steps', '20'], ['--instance_prompt', 'photo of olliedog'], ['--learning_rate', '2e-06'], ['--lr_scheduler', 'constant'], ['--lr_warmup_steps', '500'], ['--max_grad_norm', '1.0'], ['--max_train_steps', '1300'], ['--mixed_precision', 'fp16'], ['--num_class_images', '100'], ['--num_train_epochs', '1'], ['--pretrained_model_name_or_path', 'CompVis/stable-diffusion-v1-4'], ['--prior_loss_weight', '1.0'], ['--resolution', '512'], ['--revision', None], ['--sample_batch_size', '1'], ['--seed', None], ['--tokenizer_name', None], ['--train_batch_size', '1'], ['--train_text_encoder', None], ['--wandb_api_key', None], ['--with_prior_preservation', None], ['--gradient_checkpointing', 'True'], ['--instance_data_dir', '/app/DreamboothSampleDataset/object'], ['--output_dir', '/output']]\n",
            "\u001b[32mINFO\u001b[0m You need not use --class_prompt without --with_prior_preservation.\n",
            "INFO:baseten.training.logs:You need not use --class_prompt without --with_prior_preservation.\n",
            "\u001b[32mINFO\u001b[0m Namespace(pretrained_model_name_or_path='CompVis/stable-diffusion-v1-4', revision=None, hf_access_token=None, tokenizer_name=None, instance_data_dir='/app/DreamboothSampleDataset/object', class_data_dir=None, instance_prompt='photo of olliedog', class_prompt='photo of dog', with_prior_preservation=False, prior_loss_weight=1.0, num_class_images=100, output_dir='/output', seed=None, resolution=512, center_crop=False, train_text_encoder=False, train_batch_size=1, sample_batch_size=1, num_train_epochs=1, max_train_steps=1300, save_steps=500, gradient_accumulation_steps=1, gradient_checkpointing=True, learning_rate=2e-06, lr_scheduler='constant', lr_warmup_steps=500, adam_beta1=0.9, adam_beta2=0.999, adam_weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, logging_dir='logs', mixed_precision='fp16', local_rank=-1, report_to='tensorboard', wandb_api_key=None, image_log_steps=20)\n",
            "INFO:baseten.training.logs:Namespace(pretrained_model_name_or_path='CompVis/stable-diffusion-v1-4', revision=None, hf_access_token=None, tokenizer_name=None, instance_data_dir='/app/DreamboothSampleDataset/object', class_data_dir=None, instance_prompt='photo of olliedog', class_prompt='photo of dog', with_prior_preservation=False, prior_loss_weight=1.0, num_class_images=100, output_dir='/output', seed=None, resolution=512, center_crop=False, train_text_encoder=False, train_batch_size=1, sample_batch_size=1, num_train_epochs=1, max_train_steps=1300, save_steps=500, gradient_accumulation_steps=1, gradient_checkpointing=True, learning_rate=2e-06, lr_scheduler='constant', lr_warmup_steps=500, adam_beta1=0.9, adam_beta2=0.999, adam_weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, logging_dir='logs', mixed_precision='fp16', local_rank=-1, report_to='tensorboard', wandb_api_key=None, image_log_steps=20)\n",
            "\u001b[32mINFO\u001b[0m /usr/local/lib/python3.9/dist-packages/accelerate/accelerator.py:231: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.\n",
            "INFO:baseten.training.logs:/usr/local/lib/python3.9/dist-packages/accelerate/accelerator.py:231: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.\n",
            "\u001b[32mINFO\u001b[0m   warnings.warn(\n",
            "INFO:baseten.training.logs:  warnings.warn(\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 806/806 [00:00<00:00, 234kB/s]\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 806/806 [00:00<00:00, 234kB/s]\n",
            "Downloading (…)tokenizer/vocab.json: 100%|██████████| 1.06M/1.06M [00:00<00:00, 2.92MB/s]\n",
            "Downloading (…)tokenizer/vocab.json: 100%|██████████| 1.06M/1.06M [00:00<00:00, 2.92MB/s]\n",
            "Downloading (…)tokenizer/merges.txt: 100%|██████████| 525k/525k [00:00<00:00, 2.01MB/s]\n",
            "Downloading (…)tokenizer/merges.txt: 100%|██████████| 525k/525k [00:00<00:00, 2.01MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 472/472 [00:00<00:00, 331kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 472/472 [00:00<00:00, 331kB/s]\n",
            "Downloading (…)_encoder/config.json: 100%|██████████| 592/592 [00:00<00:00, 204kB/s]\n",
            "Downloading (…)_encoder/config.json: 100%|██████████| 592/592 [00:00<00:00, 204kB/s]\n",
            "\u001b[32mINFO\u001b[0m You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "INFO:baseten.training.logs:You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 492M/492M [00:03<00:00, 155MB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 492M/492M [00:03<00:00, 155MB/s]\n",
            "Downloading (…)_pytorch_model.bin\";: 100%|██████████| 335M/335M [00:02<00:00, 144MB/s]\n",
            "Downloading (…)_pytorch_model.bin\";: 100%|██████████| 335M/335M [00:02<00:00, 144MB/s]\n",
            "Downloading (…)main/vae/config.json: 100%|██████████| 551/551 [00:00<00:00, 388kB/s]\n",
            "Downloading (…)main/vae/config.json: 100%|██████████| 551/551 [00:00<00:00, 388kB/s]\n",
            "Downloading (…)_pytorch_model.bin\";: 100%|██████████| 3.44G/3.44G [00:20<00:00, 167MB/s]\n",
            "Downloading (…)_pytorch_model.bin\";: 100%|██████████| 3.44G/3.44G [00:20<00:00, 167MB/s]\n",
            "Downloading (…)ain/unet/config.json: 100%|██████████| 743/743 [00:00<00:00, 254kB/s]\n",
            "Downloading (…)ain/unet/config.json: 100%|██████████| 743/743 [00:00<00:00, 254kB/s]\n",
            "Downloading (…)cheduler_config.json: 100%|██████████| 313/313 [00:00<00:00, 101kB/s]\n",
            "Downloading (…)cheduler_config.json: 100%|██████████| 313/313 [00:00<00:00, 101kB/s]\n",
            "S\n",
            "S\n",
            "Steps:  10%|█         | 134/130\n",
            "Steps:  10%|█         | 134/130\n",
            "Steps:  15%|█▌        | 198/1300 [01:53<10:22,  1.77it/s, train/epoch=1, train/loss=0.0259, train/lr=2e-\n",
            "Steps:  15%|█▌        | 198/1300 [01:53<10:22,  1.77it/s, train/epoch=1, train/loss=0.0259, train/lr=2e-\n",
            "Steps:  20%|��\n",
            "Steps:  20%|��\n",
            "Steps:  25%|██▌       | 327/1300 [03:06<09:08,  1.77it/s, train/epoch=2, train/loss=0.0763, train/lr=2e-6, tra\n",
            "Steps:  25%|██▌       | 327/1300 [03:06<09:08,  1.77it/s, train/epoch=2, train/loss=0.0763, train/lr=2e-6, tra\n",
            "Steps:  30%|\n",
            "Steps:  30%|\n",
            "Steps:  35%|███▍      | 453/1300 [04:17<07:56,  1.78it/s, train/epoch=3, train/loss=0.00417, train/lr=2e-6, \n",
            "Steps:  35%|███▍      | 453/1300 [04:17<07:56,  1.78it/s, train/epoch=3, train/loss=0.00417, train/lr=2e-6, \n",
            "Steps:  40%|███▉\n",
            "Steps:  40%|███▉\n",
            "Steps:  44%|████▍  \n",
            "Steps:  44%|████▍  \n",
            "Steps:  49%|████▉     | 640/1300 [06:03<06:12,  1.77it/s, trai\n",
            "Steps:  49%|████▉     | 640/1300 [06:03<06:12,  1.77it/s, trai\n",
            "Steps:  54%|█████▍    | 701/1300 [06:38<05:39,  1.76it/s, train/epoch=6, train/loss=0.235, tr\n",
            "Steps:  54%|█████▍    | 701/1300 [06:38<05:39,  1.76it/s, train/epoch=6, train/loss=0.235, tr\n",
            "Steps:  59%|█████▊    | 762/1300 [07:12\n",
            "Steps:  59%|█████▊    | 762/1300 [07:12\n",
            "\u001b[31mERROR\u001b[0m GraphQL endpoint failed with error: b'upstream connect error or disconnect/reset before headers. reset reason: connection termination'\n",
            "ERROR:baseten.common.api:GraphQL endpoint failed with error: b'upstream connect error or disconnect/reset before headers. reset reason: connection termination'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rℹ Something went wrong with your connection to Baseten.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploy your fine-tuned model"
      ],
      "metadata": {
        "id": "kwbbn-06w5qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After completion, deploy your model \n",
        "model = fr.deploy()"
      ],
      "metadata": {
        "id": "o6_tGBSwMe43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke your model \n",
        "image, s3_url = model(\"photo of sks dog\")"
      ],
      "metadata": {
        "id": "VaPkFZd_aT6N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}